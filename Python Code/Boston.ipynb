{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Multiple linear regression with statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "--> \"Pandas\" is mainly a package to handle and operate directly on data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> \"Scikit-learn\" is doing machine learning with emphasis on predictive modeling with often large and sparse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> \"Statsmodels\" is doing \"traditional\" statistics and econometrics, with much stronger emphasis on parameter estimation and (statistical) testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a linear regression model from the provided data , loading the data.\n",
    "my_dt=pd.read_csv(\"boston.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PT</th>\n",
       "      <th>B</th>\n",
       "      <th>MV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.199997</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>296</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>396.899994</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.900002</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>242</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>396.899994</td>\n",
       "      <td>21.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.099998</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>242</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>392.829987</td>\n",
       "      <td>34.700001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.799999</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>222</td>\n",
       "      <td>18.700001</td>\n",
       "      <td>394.630005</td>\n",
       "      <td>33.400002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.200001</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>222</td>\n",
       "      <td>18.700001</td>\n",
       "      <td>396.899994</td>\n",
       "      <td>36.200001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM  INDUS    NOX     RM        AGE     DIS  TAX         PT  \\\n",
       "0  0.00632   2.31  0.538  6.575  65.199997  4.0900  296  15.300000   \n",
       "1  0.02731   7.07  0.469  6.421  78.900002  4.9671  242  17.799999   \n",
       "2  0.02729   7.07  0.469  7.185  61.099998  4.9671  242  17.799999   \n",
       "3  0.03237   2.18  0.458  6.998  45.799999  6.0622  222  18.700001   \n",
       "4  0.06905   2.18  0.458  7.147  54.200001  6.0622  222  18.700001   \n",
       "\n",
       "            B         MV  \n",
       "0  396.899994  24.000000  \n",
       "1  396.899994  21.600000  \n",
       "2  392.829987  34.700001  \n",
       "3  394.630005  33.400002  \n",
       "4  396.899994  36.200001  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the top 5 rows of the imported data of all columns.\n",
    "my_dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_1:(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Split arrays or matrices into random train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into train (75%) and test(25%) of dependent and independent variables with random_state(seed) as 999\n",
    "#Provided:\n",
    "        #y: Single column dataframe holding our solution vector for linear regression\n",
    "        #x: A dataframe that runs parallel to y, with all the features for our linear regression\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(my_dt.drop(\"MV\",axis=1),my_dt[\"MV\"],test_size=0.25,random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_constant: a boolean, if true it will add a constant row to our provided x data. Otherwise this method assumes you've done-so already, or do not want one for some good reason\n",
    "#If add_constant was true this will be a new dataframe, otherwise it will be X_train\n",
    "X_train = sm.add_constant(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Linear model using 'statsmodels.OLS' as 'sm.OLS'.\n",
    "my_model = sm.OLS(Y_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model fit object obtained from a linear model trained using 'sm.OLS'.\n",
    "result = my_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     MV   R-squared:                       0.658\n",
      "Model:                            OLS   Adj. R-squared:                  0.650\n",
      "Method:                 Least Squares   F-statistic:                     78.94\n",
      "Date:                Sun, 21 Apr 2019   Prob (F-statistic):           1.74e-80\n",
      "Time:                        22:00:11   Log-Likelihood:                -1186.3\n",
      "No. Observations:                 379   AIC:                             2393.\n",
      "Df Residuals:                     369   BIC:                             2432.\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         20.5247      6.386      3.214      0.001       7.967      33.083\n",
      "CRIM          -0.1366      0.039     -3.493      0.001      -0.213      -0.060\n",
      "INDUS         -0.0983      0.085     -1.157      0.248      -0.265       0.069\n",
      "NOX          -20.5227      5.084     -4.037      0.000     -30.519     -10.526\n",
      "RM             6.4043      0.463     13.846      0.000       5.495       7.314\n",
      "AGE           -0.0568      0.017     -3.311      0.001      -0.090      -0.023\n",
      "DIS           -1.6018      0.263     -6.095      0.000      -2.119      -1.085\n",
      "TAX            0.0023      0.003      0.721      0.472      -0.004       0.009\n",
      "PT            -1.1159      0.164     -6.800      0.000      -1.439      -0.793\n",
      "B              0.0124      0.004      3.406      0.001       0.005       0.020\n",
      "==============================================================================\n",
      "Omnibus:                      241.104   Durbin-Watson:                   2.098\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2706.372\n",
      "Skew:                           2.535   Prob(JB):                         0.00\n",
      "Kurtosis:                      15.069   Cond. No.                     1.44e+04\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.44e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#Entire summary of the model cosists of Constants,coef,std err, t , p value, R-squared,Adj R-squared ,AIC,BIC,etc\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const    491.267613\n",
      "CRIM       1.565644\n",
      "INDUS      4.001190\n",
      "NOX        4.048097\n",
      "RM         1.347950\n",
      "AGE        2.802511\n",
      "DIS        3.496721\n",
      "TAX        3.429197\n",
      "PT         1.534356\n",
      "B          1.273864\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# we need to import VIF to check or verify the multi collinearity between the variables \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "multicollinearity_model1=pd.Series([variance_inflation_factor(X_train.values, i) \n",
    "               for i in range(X_train.shape[1])], \n",
    "              index=X_train.columns)\n",
    "print(multicollinearity_model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "The most apparent difference is that the VIFs are all down to satisfactory values, they are all less than 5. we can see that there is some multicollinearity in our data, but it is not severe enough to warrant further corrective measures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Accuracy score test for model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the accuracy score of the model we need to import sklearn.metrics.\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the trained model with the test data \n",
    "predictions_model1 = result.predict(sm.add_constant(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6727577927397874"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R-square value score of the prediction_model1 \n",
    "r2_score(Y_test, predictions_model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Significance of the predictors from the model_1 summary.\n",
    "\n",
    "The p-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (< 0.05) indicates that you can reject the null hypothesis. In other words, a predictor that has a low p-value is likely to be a meaningful addition to your model because changes in the predictor's value are related to changes in the response variable.\n",
    "\n",
    "In the output Above, we can see that the predictor variables of INDUS and TAX are insignificant because both of their p-values are above 0.05 which indicates that it is not statistically significant.\n",
    "\n",
    "R-squared value     : 0.658 or 65.8%\n",
    "Adj R-squared value : 0.650 or 65.0%\n",
    "AIC                 : 2393.\n",
    "BIC                 : 2432.\n",
    "\n",
    "Accuracy of test score (R-score) : 67.2%\n",
    "\n",
    "# Note: \n",
    "From the accuracy values of train and test models, test score accuracy is 1.5% more than train accuracy.so, we can say that our model is good trained with the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_2 :(Retrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of any variable selection is to removed unnecessary covariates with negligible contribution and \n",
    "# to removed correlated covariates so that the remaining covariates provide as much independent information \n",
    "# about the response as possible.\n",
    "# Dropping the variables which have p-value of the exponential tail fit is insignificant which have value Greater than 0.05 \n",
    "\n",
    "X_train=X_train.drop([\"INDUS\",\"TAX\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrain the Linear model using 'sm.OLS' with only significant variables.\n",
    "retrained_model = sm.OLS(Y_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model fit object obtained from a linear model retrained_model using 'sm.OLS'.\n",
    "retrained_result=retrained_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     MV   R-squared:                       0.657\n",
      "Model:                            OLS   Adj. R-squared:                  0.650\n",
      "Method:                 Least Squares   F-statistic:                     101.5\n",
      "Date:                Sun, 21 Apr 2019   Prob (F-statistic):           3.41e-82\n",
      "Time:                        22:00:20   Log-Likelihood:                -1187.0\n",
      "No. Observations:                 379   AIC:                             2390.\n",
      "Df Residuals:                     371   BIC:                             2421.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         20.1223      6.338      3.175      0.002       7.660      32.584\n",
      "CRIM          -0.1272      0.036     -3.494      0.001      -0.199      -0.056\n",
      "NOX          -21.5475      4.499     -4.789      0.000     -30.395     -12.700\n",
      "RM             6.5225      0.450     14.492      0.000       5.637       7.407\n",
      "AGE           -0.0579      0.017     -3.395      0.001      -0.091      -0.024\n",
      "DIS           -1.5216      0.252     -6.029      0.000      -2.018      -1.025\n",
      "PT            -1.1218      0.150     -7.501      0.000      -1.416      -0.828\n",
      "B              0.0122      0.004      3.427      0.001       0.005       0.019\n",
      "==============================================================================\n",
      "Omnibus:                      245.754   Durbin-Watson:                   2.099\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2864.882\n",
      "Skew:                           2.588   Prob(JB):                         0.00\n",
      "Kurtosis:                      15.435   Cond. No.                     9.27e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 9.27e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#Entire summary of the model cosists of Constants,coef,std err, t , p value, R-squared,Adj R-squared ,AIC,BIC,etc\n",
    "print(retrained_result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const    484.597701\n",
      "CRIM       1.360539\n",
      "NOX        3.176053\n",
      "RM         1.278335\n",
      "AGE        2.782281\n",
      "DIS        3.230089\n",
      "PT         1.276660\n",
      "B          1.225698\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# we need to import VIF to check or verify the multi collinearity between the variables \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "multicollinearity_model2=pd.Series([variance_inflation_factor(X_train.values, i) \n",
    "               for i in range(X_train.shape[1])], \n",
    "              index=X_train.columns)\n",
    "print(multicollinearity_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "The most apparent difference is that the VIFs are all down to satisfactory values, they are all less than 5. we can see that there is some multicollinearity in our data, but it is not severe enough to warrant further corrective measures.\n",
    "\n",
    "The VIF of the variables decreased compare to the model_1\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:Multi-Collinearity\n",
    "Now, take a look at the Summary of Model tables for both models. You’ll notice that the standard error of the regression (S), R-squared, adjusted R-squared, and predicted R-squared are all identical. As I mentioned earlier, multicollinearity doesn’t affect the predictions or goodness-of-fit. If you just want to make predictions, the model with severe multicollinearity is just as good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy score test for model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the accuracy score of the model we need to import sklearn.metrics.\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to drop the variables which we actually dropped from the training data set which are insignificant\n",
    "X_test = X_test.drop([\"INDUS\",\"TAX\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the trained model with the test data \n",
    "prediction_model2 = retrained_result.predict(sm.add_constant(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6709891731985471"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R-square value score of the prediction_model1 \n",
    "r2_score(Y_test, prediction_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Significance of the predictors from the model_2 summary.\n",
    "\n",
    "From the above model summary all the variables are significant where p value < 0.05.\n",
    "\n",
    "R-squared score     = 0.657 or 65.7%\n",
    "\n",
    "Adj R-squared score = 0.650 or 65.0%\n",
    "\n",
    "AIC                 = 2390.\n",
    "\n",
    "BIC                 = 2421.\n",
    "\n",
    "**Adj R-squared remained constant in both the models as 65%\n",
    "\n",
    "**Accuracy of test score (R-score) : 67.0%\n",
    "\n",
    "# Note: \n",
    "From the accuracy values of train and test models, test score accuracy is 1.3% more than train accuracy.so, we can say that our model is good trained with the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justified reason for dropping or not dropping a predictor?\n",
    "\n",
    "From the above models with and without significants we observed :\n",
    "\n",
    "**Dropping the insignificants variables every time is not a good idea and depends on the business requirement because they may be not totally independent with other variables.\n",
    "\n",
    "**R-squared values of model_1=0.658(65.8%)and R-squared value of model_2=0.657(65.7%) . here, after dropping of 2 columns (INDUS,TAX) as they are not significant with respect to p-value .we can see the decrease in R-squared value of 0.1%.so, there are few factors influence on those two columns also. \n",
    "\n",
    "**Drop of columns should result in increasse in R-squared value but here there is a decrease in R-squared value of(0.1%)\n",
    "\n",
    "**Accuracy R-square score on test data of both models (model-1=67.2%, model-2=67%) ,from this scores model_1 have 0.2%  higher Accuracy Score with test result than  model_2(after removing insignificant variables) test results.\n",
    "\n",
    "**Adj R-squared remained constant in both the models as 65% \n",
    "\n",
    "**As there is decrease in AIC,BIC,Multi-collinearity also but they don't effect in prediction outputs.\n",
    "\n",
    "**Finally, Model without not dropping the insignificant variable give more accurate predictor for new inputs.\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
